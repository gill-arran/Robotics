{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ceda1498-8235-48aa-857a-d698b57bf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5a1f494-8c0d-473e-938e-87510f4871b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ad2cbe39824b17a8156f6130745df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 11:38:16 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:38:16 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:38:16 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:38:17 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-05-07 11:38:18 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-05-07 11:38:18 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-05-07 11:38:18 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-05-07 11:38:18 UTC][ZED][INFO] [Init]  Serial Number: S/N 32709812\n",
      "305e"
     ]
    }
   ],
   "source": [
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "import motors\n",
    "import time\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from traitlets.config.configurable import HasTraits\n",
    "\n",
    "LIGHT_YELLOW = np.array([15, 50, 50])\n",
    "DARK_YELLOW = np.array([36, 255, 255])\n",
    "\n",
    "SCALE = 1\n",
    "FWD_MOTOR_SPEED = 1\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "\n",
    "\n",
    "class Camera(HasTraits):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    cx, cy = traitlets.Any(), traitlets.Any()\n",
    "    last_dir = None\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "        self.viewport_midpoint = 380\n",
    "        self.viewport_width = 672\n",
    "        self.last_dir = ''\n",
    "        self.gradient = 1\n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "           \n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "\n",
    "                frame = self.image.get_data()\n",
    "                #Shades lower segment\n",
    "                # frame[330:,:] = 0\n",
    "                #Shades the Upper segment\n",
    "                frame[:210,:] = 0\n",
    "                #Shades the Left Segment\n",
    "                frame[:,:(self.viewport_midpoint - (self.viewport_width // 2))] = 0\n",
    "                #Shades the Right Segment\n",
    "                frame[:,(self.viewport_midpoint + (self.viewport_width // 2)):] = 0\n",
    "                frame = cv2.resize(frame, None, fx=SCALE, fy=SCALE, interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "                mask = cv2.inRange(hsv, LIGHT_YELLOW, DARK_YELLOW)\n",
    "                kernel = np.ones((5,5), np.uint8)\n",
    "                mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "                _, edges = canny_edge_detection(frame)\n",
    "                yellow_lines = cv2.bitwise_and(mask, edges)\n",
    "                \n",
    "                contours, _ = cv2.findContours(yellow_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "                if contours:\n",
    "\n",
    "                    all_points = np.vstack(contours)\n",
    "                    xs = all_points[:, 0, 1]\n",
    "                    ys = all_points[:, 0, 1]\n",
    "\n",
    "                    top_point_index = np.argmin(ys)\n",
    "                    left_point_index = np.argmin(xs)\n",
    "                    right_point_index = np.argmin(xs)\n",
    "                    \n",
    "                    top_point = all_points[top_point_index, 0]\n",
    "                    left_point = all_points[left_point_index, 0]\n",
    "                    right_point = all_points[right_point_index, 0]\n",
    "\n",
    "                    max_vector_mag = -np.inf\n",
    "                    \n",
    "                    if np.sqrt((top_point[0] - self.viewport_midpoint)**2 + (top_point[1] - 376)**2) > max_vector_mag:\n",
    "                        max_vector_mag = np.sqrt((top_point[0] - self.viewport_midpoint)**2 + (top_point[1] - 376)**2)\n",
    "                        point_used = top_point\n",
    "\n",
    "                        if point_used[0] - self.viewport_midpoint > 0:\n",
    "                            self.last_dir = 'right'\n",
    "                        else:\n",
    "                            self.last_dir = 'left'\n",
    "                        \n",
    "                    if np.sqrt((left_point[0] - self.viewport_midpoint)**2 + (left_point[1] - 376)**2) > max_vector_mag:\n",
    "                        max_vector_mag = np.sqrt((left_point[0] - self.viewport_midpoint)**2 + (left_point[1] - 376)**2)\n",
    "                        point_used = left_point\n",
    "                        self.last_dir = 'left'\n",
    "                        \n",
    "                    if np.sqrt((right_point[0] - self.viewport_midpoint)**2 + (right_point[1] - 376)**2) > max_vector_mag:\n",
    "                        max_vector_mag = np.sqrt((right_point[0] - self.viewport_midpoint)**2 + (right_point[1] - 376)**2)\n",
    "                        point_used = right_point\n",
    "                        self.last_dir = 'right'\n",
    "\n",
    "                    \n",
    "                    self.cx = point_used[0]\n",
    "                    self.cy = point_used[1]\n",
    "\n",
    "                    cv2.circle(frame, (self.cx, self.cy), 5, (0, 255, 0), -1)\n",
    "                    cv2.line(frame, (self.viewport_midpoint, 376), top_point, (0, 0, 255), 5)\n",
    "                    cv2.line(frame, (self.viewport_midpoint, 376), left_point, (255, 0, 0), 5)\n",
    "                    cv2.line(frame, (self.viewport_midpoint, 376), right_point, (0, 0, 255), 5)\n",
    "                    print(self.cx, end='\\r')\n",
    "                else:\n",
    "                    self.cx = None\n",
    "\n",
    "                self.color_value = self.image.get_data()\n",
    "                display_color.value = bgr8_to_jpeg(frame)\n",
    "                display_depth.value = bgr8_to_jpeg(yellow_lines)\n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread\n",
    "            self.zed.close()\n",
    "\n",
    "    @traitlets.observe('color_value')\n",
    "    def handleFrameUpdate(self, change):\n",
    "        frame = change['new']\n",
    "        scale = 0.1\n",
    "        resized_image = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_color.value = bgr8_to_jpeg(resized_image)\n",
    "\n",
    "    @traitlets.observe('cx')\n",
    "    def handleMotion(self, change):\n",
    "        temp = change['new']\n",
    "        \n",
    "        if temp is None:\n",
    "            if self.last_dir == \"right\":\n",
    "                robot.right(1)\n",
    "                pass\n",
    "\n",
    "            elif self.last_dir == \"left\":\n",
    "                robot.left(1)\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            \n",
    "            difference = (temp - self.viewport_midpoint) / (self.viewport_width / 2)\n",
    "            if difference < 0:\n",
    "                fr = 1\n",
    "                br = 1\n",
    "                fl = max(1 - abs(difference) * 2, -1)\n",
    "                bl = (fl + fr) / 2\n",
    "            else:\n",
    "                fl = 1\n",
    "                bl = 1\n",
    "                fr = max(1 - abs(difference) * 2, -1)\n",
    "                br = (fl + fr) / 2\n",
    "\n",
    "            robot.frontLeft(fl)\n",
    "            robot.backLeft(bl)\n",
    "            robot.frontRight(fr)\n",
    "            robot.backRight(br)\n",
    "        print(temp, end='\\r')\n",
    "        \n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "def canny_edge_detection(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(src=gray, ksize=(3,5), sigmaX=0.5)\n",
    "    edges = cv2.Canny(blurred, 70, 125)\n",
    "    return blurred, edges\n",
    "\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4bd3b7f-0ea1-42cb-a627-2913e2d8e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc13f0-3fe5-4328-9c40-43ad017692c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
