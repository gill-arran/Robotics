{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the ZED2i Camera system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd20e20ff3914b3888f969bb5c673ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 12:11:09 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 12:11:09 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 12:11:09 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 12:11:10 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-05-07 12:11:11 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-05-07 12:11:11 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-05-07 12:11:11 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-05-07 12:11:11 UTC][ZED][INFO] [Init]  Serial Number: S/N 35159485\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[05/07/2025-13:11:11] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[05/07/2025-13:11:11] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[05/07/2025-13:11:11] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[05/07/2025-13:11:11] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 20, GPU 1178 (MiB)\n",
      "NEW TARGET: 858.912841796875 \n",
      "NEW TARGET: 857.912841796875 \n",
      "NEW TARGET: 1014.912841796875 \n",
      "NEW TARGET: 1146.912841796875 \n",
      "NEW TARGET: 1147.912841796875 \n",
      "NEW TARGET: 1104.912841796875 \n",
      "NEW TARGET: 1153.912841796875 \n",
      "NEW TARGET: 1387.91357421875 \n",
      "NEW TARGET: 1251.91357421875 \n",
      "NEW TARGET: 1313.91357421875 \n",
      "NEW TARGET: 1301.91357421875 \n",
      "NEW TARGET: 1423.91357421875 \n",
      "NEW TARGET: 1373.91357421875 \n",
      "NEW TARGET: 1478.8883056640625 \n",
      "NEW TARGET: 753.58251953125 \n",
      "NEW TARGET: 819.89013671875 \n",
      "NEW TARGET: 1535.0 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1400.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1399.33447265625 \n",
      "NEW TARGET: 1038.33447265625 \n",
      "NEW TARGET: 1130.33447265625 \n",
      "NEW TARGET: 1285.33447265625 \n",
      "NEW TARGET: 1285.33447265625 \n",
      "NEW TARGET: 1283.33447265625 \n",
      "NEW TARGET: 1336.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1406.33447265625 \n",
      "MOVING TO NEW TARGET: 1406.33447265625 \n",
      "NEW TARGET: 1282.052978515625 \n",
      "NEW TARGET: 1043.052978515625 \n",
      "NEW TARGET: 1140.052978515625 \n",
      "NEW TARGET: 1114.052978515625 \n",
      "NEW TARGET: 2109.052978515625 \n",
      "NEW TARGET: 2130.052978515625 \n",
      "NEW TARGET: 2126.052978515625 \n",
      "NEW TARGET: 2102.052978515625 \n",
      "NEW TARGET: 2193.052978515625 \n",
      "NEW TARGET: 2193.052978515625 \n",
      "NEW TARGET: 2193.052978515625 \n",
      "NEW TARGET: 2193.052978515625 \n",
      "NEW TARGET: 2083.052978515625 \n",
      "NEW TARGET: 2085.052978515625 \n",
      "NEW TARGET: 2085.052978515625 \n",
      "NEW TARGET: 2193.052978515625 \n",
      "NEW TARGET: 2193.052978515625 \n",
      "MOVING TO NEW TARGET: 2193.052978515625 \n",
      "NEW TARGET: 1229.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1244.0345458984375 \n",
      "MOVING TO NEW TARGET: 1244.0345458984375 \n",
      "NEW TARGET: 1076.0 \n",
      "NEW TARGET: 931.0 \n",
      "NEW TARGET: 2800.0 \n",
      "NEW TARGET: 2785.0 \n",
      "NEW TARGET: 2737.0 \n",
      "NEW TARGET: 2737.0 \n",
      "NEW TARGET: 2779.0 \n",
      "NEW TARGET: 2800.0 \n",
      "NEW TARGET: 2623.771484375 \n",
      "NEW TARGET: 2770.771484375 \n",
      "NEW TARGET: 2775.771484375 \n",
      "NEW TARGET: 2775.771484375 \n",
      "NEW TARGET: 2715.771484375 \n",
      "NEW TARGET: 2754.771484375 \n",
      "NEW TARGET: 2759.771484375 \n",
      "NEW TARGET: 2098.771484375 \n",
      "NEW TARGET: 2104.771484375 \n",
      "NEW TARGET: 2199.771484375 \n",
      "NEW TARGET: 2417.771484375 \n",
      "NEW TARGET: 1961.771484375 \n",
      "NEW TARGET: 2486.771484375 \n",
      "NEW TARGET: 2462.771484375 \n",
      "NEW TARGET: 2239.771484375 \n",
      "MOVING TO NEW TARGET: 2239.771484375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7346/1031340411.py:146: RuntimeWarning: All-NaN slice encountered\n",
      "  bbox_depth = np.nanmin(trimmed_depth_chosen_image_cleaned)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW TARGET: 2195.77001953125 \n",
      "NEW TARGET: 893.859375 \n",
      "NEW TARGET: 1840.7021484375 \n",
      "NEW TARGET: 924.8111572265625 \n",
      "NEW TARGET: 1874.081787109375 \n",
      "NEW TARGET: 925.918212890625 \n",
      "NEW TARGET: 925.918212890625 \n",
      "NEW TARGET: 2066.001953125 \n",
      "NEW TARGET: 2728.001953125 \n",
      "NEW TARGET: 2469.001953125 \n",
      "NEW TARGET: 2436.001953125 \n",
      "NEW TARGET: 2034.001953125 \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import collections\n",
    "import motors\n",
    "from traitlets.config.configurable import HasTraits\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "TARGET_DISTANCE = 1300\n",
    "SCREEN_WIDTH = 672\n",
    "CENTER_X = SCREEN_WIDTH/2  \n",
    "\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside)\n",
    "\n",
    "class Camera(HasTraits):\n",
    "    color_value = traitlets.Any()\n",
    "    person_pos = traitlets.Any()\n",
    "    display_image = traitlets.Any()\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS:\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "\n",
    "        self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "\n",
    "        self.tracked_bbox = None\n",
    "        self.tracked_depth = 0\n",
    "        self.new_target_time = None\n",
    "        self.x_center_t = None\n",
    "        self.y_center_t = None\n",
    "\n",
    "    \n",
    "    def _capture_frames(self):\n",
    "\n",
    "        while(self.thread_runnning_flag==True):\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                \n",
    "    def start(self):\n",
    "        if self.thread_runnning_flag == False:\n",
    "            self.thread_runnning_flag=True\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False\n",
    "            self.thread.join() \n",
    "            self.zed.close()\n",
    "\n",
    "    @traitlets.observe('color_value')\n",
    "    def processFrame(self, change):\n",
    "        \n",
    "        \n",
    "\n",
    "        frame = change['new']\n",
    "        result = model(frame,verbose=False)[0]\n",
    "    \n",
    "        trim=[0,0,0,0]\n",
    "    \n",
    "        conf_threshold = .6\n",
    "        \n",
    "    \n",
    "        chosen_box = None\n",
    "        \n",
    "        for i in range (len(result.boxes.cls)):\n",
    "            if(result.boxes.cls[i] == 0):\n",
    "                if (result.boxes.conf[i] > conf_threshold):\n",
    "                    bbox = result.boxes.xyxy[i]\n",
    "                    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)   \n",
    "    \n",
    "                    if self.tracked_bbox is None:\n",
    "                        chosen_box = bbox\n",
    "                        break;\n",
    "                    \n",
    "                    if chosen_box is None:\n",
    "                        chosen_box = bbox\n",
    "                    else:\n",
    "                        x_center_c = (chosen_box[0] + chosen_box[2]) / 2\n",
    "                        y_center_c = (chosen_box[1] + chosen_box[3]) / 2\n",
    "                        x_center = (bbox[0] + bbox[2]) / 2\n",
    "                        y_center = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "                        distance_new = math.sqrt((x_center - self.x_center_t)**2 + (y_center - self.y_center_t)**2)\n",
    "                        distance_chosen = math.sqrt((x_center_c - self.x_center_t)**2 + (y_center_c - self.y_center_t)**2)\n",
    "\n",
    "                        trim = [int(bbox[1]), int(bbox[3]), int(bbox[0]), int(bbox[2])]\n",
    "                        trim[0] = int(bbox[1])\n",
    "                        trim[1] = int(bbox[3])\n",
    "                        trim[2] = int(bbox[0])\n",
    "                        trim[3] = int(bbox[2])\n",
    "                        trimmed_depth_chosen_image = camera.depth_image[trim[0]:trim[1], trim[2]:trim[3]]\n",
    "                \n",
    "                \n",
    "                        trimmed_depth_chosen_image_cleaned = np.nan_to_num(trimmed_depth_chosen_image, nan=0.0).astype(np.float32)\n",
    "                        \n",
    "                        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned == 0] = np.nan\n",
    "                \n",
    "                        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned<200]=200\n",
    "                        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned>3000]=3000\n",
    "                        bbox_depth = np.nanmin(trimmed_depth_chosen_image_cleaned)\n",
    "\n",
    "                        if distance_new < distance_chosen and bbox_depth < 3000:\n",
    "                            chosen_box = bbox\n",
    "                            \n",
    "    \n",
    "        if chosen_box is None:\n",
    "            return;\n",
    "        \n",
    "        trim = [int(chosen_box[1]), int(chosen_box[3]), int(chosen_box[0]), int(chosen_box[2])]\n",
    "        trim[0] = int(chosen_box[1])\n",
    "        trim[1] = int(chosen_box[3])\n",
    "        trim[2] = int(chosen_box[0])\n",
    "        trim[3] = int(chosen_box[2])\n",
    "        trimmed_depth_chosen_image = camera.depth_image[trim[0]:trim[1], trim[2]:trim[3]]\n",
    "\n",
    "\n",
    "        trimmed_depth_chosen_image_cleaned = np.nan_to_num(trimmed_depth_chosen_image, nan=0.0).astype(np.float32)\n",
    "        \n",
    "        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned == 0] = np.nan\n",
    "\n",
    "        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned<200]=200\n",
    "        trimmed_depth_chosen_image_cleaned[trimmed_depth_chosen_image_cleaned>3000]=3000\n",
    "        chosen_depth = np.nanmin(trimmed_depth_chosen_image_cleaned)\n",
    "        is_tracked_box = False\n",
    "        if self.tracked_bbox != None:\n",
    "            if chosen_depth is not None or not np.isnan(chosen_depth):\n",
    "                if abs(self.tracked_depth - int(chosen_depth))  > 700:\n",
    "                    print(f\"NEW TARGET: {abs(self.tracked_depth - int(chosen_depth))} \\r\")\n",
    "                    if (self.new_target_time != None and self.new_target_time < datetime.now()):\n",
    "                        self.new_target_time = None\n",
    "                        self.tracked_bbox = chosen_box\n",
    "                        is_tracked_box = True\n",
    "                        print(f\"MOVING TO NEW TARGET: {abs(self.tracked_depth - int(chosen_depth))} \\r\")\n",
    "                    elif (self.new_target_time == None):\n",
    "                        self.new_target_time = datetime.now() + timedelta(seconds=2)\n",
    "                        # pass\n",
    "                    # elif (self.new_target_time != None):\n",
    "                    #     pass\n",
    "                else:\n",
    "                    self.tracked_bbox = chosen_box\n",
    "                    is_tracked_box = True\n",
    "            else:\n",
    "                self.tracked_bbox = chosen_box\n",
    "                is_tracked_box = True\n",
    "        else:\n",
    "            self.tracked_bbox = chosen_box\n",
    "            is_tracked_box = True\n",
    "        if is_tracked_box:\n",
    "            self.new_target_time = None\n",
    "\n",
    "        # if self.timecheck != None:\n",
    "        #     print(datetime.now() - self.timecheck)\n",
    "        # self.timecheck = datetime.now()\n",
    "        self.display_image = (chosen_box, trim, is_tracked_box, frame)\n",
    "        \n",
    "\n",
    "\n",
    "    @traitlets.observe('display_image')\n",
    "    def displayImage(self, change):\n",
    "        chosen_box, trim, is_tracked_box, frame = change['new'][0], change['new'][1], change['new'][2], change['new'][3]\n",
    "        self.x_center_t = (self.tracked_bbox[0] + self.tracked_bbox[2]) / 2\n",
    "        self.y_center_t = (self.tracked_bbox[1] + self.tracked_bbox[3]) / 2\n",
    "        \n",
    "        radius = 10\n",
    "        color = (0, 255, 0)\n",
    "        thickness = 2\n",
    "        cv2.circle(frame, (int(self.x_center_t), int(self.y_center_t)), radius, color, thickness)\n",
    "        cv2.circle(frame, (int(672/2), int(168)), radius, color, thickness)\n",
    "        \n",
    "        scale = 0.1 \n",
    "    \n",
    "        resized_image = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_color.value = bgr8_to_jpeg(resized_image)\n",
    "        \n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(camera.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        \n",
    "        depth_colormap[:trim[0], :] = 0\n",
    "        depth_colormap[trim[1]:, :] = 0\n",
    "        depth_colormap[:, :trim[2]] = 0\n",
    "        depth_colormap[:, trim[3]:] = 0\n",
    "    \n",
    "    \n",
    "        resized_depth_colormap = cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_depth.value = bgr8_to_jpeg(resized_depth_colormap)\n",
    "        trimmed_depth_image = camera.depth_image[trim[0]:trim[1], trim[2]:trim[3]]\n",
    "    \n",
    "        \n",
    "        trimmed_depth_image_cleaned = np.nan_to_num(trimmed_depth_image, nan=0.0).astype(np.float32)\n",
    "        \n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned == 0] = np.nan\n",
    "    \n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned<200]=200\n",
    "        trimmed_depth_image_cleaned[trimmed_depth_image_cleaned>3000]=3000\n",
    "\n",
    "        try:\n",
    "            average_depth = np.nanmedian(trimmed_depth_image_cleaned)\n",
    "            if is_tracked_box:\n",
    "                self.tracked_depth = average_depth\n",
    "        except:\n",
    "            average_depth = np.nanmin(trimmed_depth_image_cleaned)\n",
    "            if is_tracked_box:\n",
    "                self.tracked_depth = average_depth\n",
    "            print(\"crashed\")\n",
    "        self.person_pos = (average_depth, self.x_center_t, is_tracked_box)\n",
    "        \n",
    "    @traitlets.observe('person_pos')\n",
    "    def handleMotion(self, change):\n",
    "        depth, x_pos, is_tracked_box = change['new'][0], change['new'][1], change['new'][2]\n",
    "        \n",
    "        if self.tracked_bbox != None and is_tracked_box:\n",
    "    \n",
    "            fwd_speed = (depth - TARGET_DISTANCE) / TARGET_DISTANCE\n",
    "            turn_speed = (x_pos - CENTER_X) / CENTER_X\n",
    "    \n",
    "            fwd_speed = clamp(fwd_speed*abs(fwd_speed))\n",
    "\n",
    "            turn_speed = clamp(turn_speed*abs(turn_speed))\n",
    "    \n",
    "            left_motor_speed = clamp(fwd_speed + turn_speed)\n",
    "            right_motor_speed = clamp(fwd_speed - turn_speed)\n",
    "        \n",
    "            # print(f'Speeds: left_motor_speed: {left_motor_speed:.2f} | right_motor_speed: {right_motor_speed:.2f}', end='\\r')\n",
    "\n",
    "\n",
    "            # left_motor_speed *= 2\n",
    "            # right_motor_speed *= 2\n",
    "            if (left_motor_speed > 0 and right_motor_speed > 0):\n",
    "                left_motor_speed *= 3\n",
    "                right_motor_speed *= 3\n",
    "            elif (left_motor_speed < 0 and right_motor_speed < 0):\n",
    "                left_motor_speed *= 1.75\n",
    "                right_motor_speed *= 1.75\n",
    "            \n",
    "            \n",
    "            robot.frontLeft(left_motor_speed)\n",
    "            robot.backLeft(left_motor_speed)\n",
    "            robot.frontRight(right_motor_speed)\n",
    "            robot.backRight(right_motor_speed)\n",
    "        else:\n",
    "            robot.stop()\n",
    "\n",
    "        \n",
    "        \n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "def clamp(value):\n",
    "    return max(-1, min(1, value))\n",
    "\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
